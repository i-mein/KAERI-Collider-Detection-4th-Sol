{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"DACON CNN2d Torch 01-07-2020.ipynb","provenance":[{"file_id":"https://github.com/BeomSooKim/kaeri/blob/master/200701.ipynb","timestamp":1593598063243}],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PghF7IpT2DXO","colab_type":"text"},"source":["# CNN architecture Pythorch\n","\n","- add scheduler\n","- add optimizer \n","- add metric from pytorch baseline\n","- change kernel size (5,1)\n","\n","- stratified KFold\n"]},{"cell_type":"code","metadata":{"id":"sXHpHLpX2DXP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1593606260503,"user_tz":-180,"elapsed":1116,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"3a376263-352f-4b47-a9ea-03aab46d3a41"},"source":["import pandas as pd\n","import os\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","import platform\n","plt.style.use('seaborn')\n","from datetime import datetime\n","import json\n","from sklearn.model_selection import KFold, GroupKFold, train_test_split\n","from tqdm import tqdm\n","\n","# from metric import E1_loss, E2_loss, total_loss\n","# from models import classifier, cnn_model, conv_block, cnn_parallel\n","# from utils import train_model, eval_model, dfDataset, weights_init\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn.functional as F\n","from torchsummary import summary\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"LqewB32k2DXZ","colab_type":"text"},"source":["# Helper functions"]},{"cell_type":"code","metadata":{"id":"-VWDt6cs2DXa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593606263816,"user_tz":-180,"elapsed":754,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["def fix_seed(SEED):\n","    torch.manual_seed(SEED)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(SEED)\n","\n","\n","\n","class Noise(object):\n","    def __init__(self, mu, sd, shape):\n","        self.mu = mu\n","        self.sd = sd\n","        self.shape = shape\n","    \n","    def __call__(self, x):\n","        noise = np.random.normal(self.mu, self.sd, self.shape)\n","        #noise = torch.FloatTensor(noise)\n","        return x + noise.astype(np.float32)\n","\n","class dfDataset(Dataset):\n","    def __init__(self, x, y, transform = None):\n","        self.data = x\n","        self.target = y\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return self.data.shape[0]\n","    \n","    def __getitem__(self, index):\n","        batchX, batchY = self.data[index], self.target[index]\n","        if self.transform:\n","            batchX = self.transform(batchX)\n","        return batchX, batchY\n","    \n","def weights_init(m, initializer = nn.init.kaiming_uniform_):\n","    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","        initializer(m.weight)\n","    \n","\n","class conv_bn(nn.Module):\n","    def __init__(self, i_f, o_f, fs):\n","        super(conv_bn, self).__init__()\n","        self.conv = nn.Conv2d(i_f, o_f, fs)\n","        self.act = nn.ELU()\n","        self.bn = nn.BatchNorm2d(o_f)\n","        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride= (2, 1))\n","    def forward(self, x):\n","        x = self.bn(self.act(self.conv(x)))\n","        return self.pool(x)  #return x\n","    \n","\n","class conv_block(nn.Module):\n","    def __init__(self, h_list, input_shape, fs):\n","        '''\n","        input_shape : not include batch_size\n","        '''\n","        \n","        super(conv_block, self).__init__()\n","        self.input_shape = input_shape\n","        self.fs = fs\n","        convs = []\n","        for i in range(len(h_list)):\n","            if i == 0:\n","                convs.append(conv_bn(self.input_shape[0], h_list[i], fs))\n","            else:\n","                convs.append(conv_bn(h_list[i-1], h_list[i], fs))\n","        self.convs = nn.Sequential(*convs)\n","    \n","    def forward(self, x):\n","        return self.convs(x)\n","    \n","\n","class classifier(nn.Module):\n","    def __init__(self, h_list, input_size, output_size):\n","        super(classifier, self).__init__()\n","        layers = []\n","        for i in range(len(h_list)):\n","            if i == 0:\n","                layers.append(nn.Linear(input_size, h_list[0]))\n","            else:\n","                layers.append(nn.Linear(h_list[i-1], h_list[i]))\n","            layers.append(nn.ELU())\n","            \n","        layers.append(nn.Linear(h_list[i], output_size))\n","        self.layers = nn.Sequential(*layers)\n","    \n","    def forward(self, x):\n","        return self.layers(x)\n","    \n","class cnn_model(nn.Module):\n","    def __init__(self, cnn_block, fc_block):\n","        super(cnn_model, self).__init__()\n","        self.cnn = cnn_block\n","        self.fc = fc_block\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        x = x.flatten(start_dim = 1)\n","        return self.fc(x)\n","\n","\n","def E1_loss(y_pred, y_true):\n","    _t, _p = y_true, y_pred\n","    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n","\n","\n","def E2_loss(y_pred, y_true):\n","    _t, _p = y_true, y_pred    \n","    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"q-ZPG3hT2DXe","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593606266711,"user_tz":-180,"elapsed":807,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["class custom_cnn(nn.Module):\n","    def __init__(self, k_=(4,1)):\n","        super(custom_cnn, self).__init__()\n","        self.fe = nn.Sequential(\n","            nn.Conv2d(in_channels=2, out_channels=16, kernel_size=k_, stride=1),\n","            nn.BatchNorm2d(16),\n","            nn.LeakyReLU(0.1),\n","            nn.MaxPool2d(kernel_size = (2,1)),\n","            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=k_, stride=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(0.1),\n","            nn.MaxPool2d(kernel_size = (2,1)),\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=k_, stride=1),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.1),\n","            nn.MaxPool2d(kernel_size = (2,1)),\n","            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=k_, stride=1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.1),\n","            nn.MaxPool2d(kernel_size = (2,1)),\n","            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=k_, stride=1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.1),\n","            nn.MaxPool2d(kernel_size = (2,1)),\n","            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=k_, stride=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.1),\n","            nn.MaxPool2d(kernel_size = (2,1))\n","        )\n","    def forward(self, x):\n","        return self.fe(x)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEt0j0wy2DXm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593606267412,"user_tz":-180,"elapsed":741,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["class custom_fc(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(custom_fc, self).__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(input_dim, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, output_dim)\n","        )\n","        \n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)\n","        return self.fc(x)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"9E1IqIje2DXx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593606268389,"user_tz":-180,"elapsed":527,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["def normalize(x, axis = 2):\n","    mu = np.expand_dims(x.mean(axis = 2), axis = axis)\n","    sd = np.expand_dims(x.std(axis = 2), axis = axis)\n","\n","    normalized = (x - mu) / sd\n","    return normalized"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBDqB3y1AAaZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593606525160,"user_tz":-180,"elapsed":584,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["import random\n","from collections import Counter, defaultdict\n","\n","def stratified_group_k_fold(X, y, groups, k, seed=None):\n","    \n","    \"\"\" https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation \"\"\"\n","\n","    labels_num = np.max(y) + 1\n","    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n","    y_distr = Counter()\n","    for label, g in zip(y, groups):\n","        y_counts_per_group[g][label] += 1\n","        y_distr[label] += 1\n","\n","    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n","    groups_per_fold = defaultdict(set)\n","\n","    def eval_y_counts_per_fold(y_counts, fold):\n","        y_counts_per_fold[fold] += y_counts\n","        std_per_label = []\n","        for label in range(labels_num):\n","            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n","            std_per_label.append(label_std)\n","        y_counts_per_fold[fold] -= y_counts\n","        return np.mean(std_per_label)\n","    \n","    groups_and_y_counts = list(y_counts_per_group.items())\n","    random.Random(seed).shuffle(groups_and_y_counts)\n","    for g, y_counts in tqdm(sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])), total=len(groups_and_y_counts)):\n","        best_fold = None\n","        min_eval = None\n","        for i in range(k):\n","            fold_eval = eval_y_counts_per_fold(y_counts, i)\n","            if min_eval is None or fold_eval < min_eval:\n","                min_eval = fold_eval\n","                best_fold = i\n","        y_counts_per_fold[best_fold] += y_counts\n","        groups_per_fold[best_fold].add(g)\n","\n","    all_groups = set(groups)\n","    for i in range(k):\n","        train_groups = all_groups - groups_per_fold[i]\n","        test_groups = groups_per_fold[i]\n","\n","        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n","        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n","\n","        yield train_indices, test_indices\n","\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"azW4679cCbue","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593606272724,"user_tz":-180,"elapsed":676,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["def kaeri_score(y_true, y_pred):\n","    if train_target == 'xy':\n","        return E1_loss(y_true, y_pred)\n","    elif train_target == 'mv':\n","        return E2_loss(y_true, y_pred)\n","    else:\n","        return 0.5 * E1_loss(y_true, y_pred) + 0.5 * E2_loss(y_true, y_pred)\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        \n","class KaeriMeter(object):\n","    def __init__(self):\n","        self.reset()\n","        \n","    def reset(self):\n","        self.y_true = np.array([0,0,0,0])\n","        self.y_pred = np.array([0.5,0.5,0.5,0.5])\n","        self.score = 0\n","        \n","    def update(self, y_true, y_pred):\n","\n","        self.y_true = y_true.detach().cpu()\n","        self.y_pred = y_pred.detach().cpu()\n","        \n","        self.score = kaeri_score(self.y_true, self.y_pred)\n","    \n","    @property\n","    def avg(self):\n","        return self.score"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hf4WM5vxF9Yb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593606274233,"user_tz":-180,"elapsed":710,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["def train_model(model, train_data, weight, optimizer, loss_func):\n","    model.train()\n","    final_scores = KaeriMeter()\n","    loss_sum = 0\n","    for i, (x, y) in enumerate(train_data):\n","        optimizer.zero_grad()\n","        x = x.cuda()\n","        y = y.cuda()\n","        pred = model(x)\n","        loss = loss_func(pred, y)\n","        loss.backward()\n","        final_scores.update(y, pred)\n","        optimizer.step()\n","        loss_sum += loss.item()\n","    \n","    return (loss_sum / len(train_data)), final_scores\n","\n","def eval_model(model, val_data, loss_func):\n","    model.eval()\n","\n","    final_scores = KaeriMeter()\n","\n","    with torch.no_grad():\n","        loss = 0\n","        for i, (x, y) in enumerate(val_data):\n","            x = x.cuda()\n","            y = y.cuda()\n","            pred = model(x)\n","            loss += loss_func(pred, y).item()\n","            final_scores.update(y, pred)\n","\n","    return (loss / len(val_data)), final_scores"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xCoog2ZF2DXr","colab_type":"text"},"source":["# Master Params\n"]},{"cell_type":"code","metadata":{"id":"gVelgOT25mOi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1593598991937,"user_tz":-180,"elapsed":20778,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"1649949d-57e9-4dc9-c442-ed659e5834da"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TaBZ1I872DXr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593606346177,"user_tz":-180,"elapsed":661,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["COLAB = True\n","\n","SEED = 34\n","fix_seed(SEED)\n","\n","# now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n","save_path = '/content/drive/My Drive/ML Projects/DACON/Colision/models_torch_4'      # './model/{}'.format(now)\n","\n","initialize = True\n","print_summary = True\n","\n","nfold = 5\n","batch_size = 256\n","EPOCH = 200\n","base_lr = 0.001"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"610ZHG2X2DXv","colab_type":"text"},"source":["# Load dataset"]},{"cell_type":"code","metadata":{"id":"gWi8HlRb2DX2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593606348769,"user_tz":-180,"elapsed":1584,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["if COLAB:\n","    root_dir = '/content/drive/My Drive/ML Projects/DACON/Colision/data/'  \n","# else: root_dir = 'D:/datasets/KAERI_dataset/'\n","\n","\n","train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n","train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n","test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n","\n","train_f = train_f[['Time','S1','S2','S3','S4']].values.reshape((-1, 1, 375, 5))#.astype(np.float32)\n","\n","test_f = test_f[['Time','S1','S2','S3','S4']].values.reshape((-1, 1, 375, 5))#.astype(np.float32)\n","\n","\n","# concatenate normalized data\n","train_norm = normalize(train_f)\n","test_norm = normalize(test_f)\n","\n","train_f = np.concatenate((train_f, train_norm), axis = 1)\n","test_f = np.concatenate((test_f, test_norm), axis = 1)\n","\n","test_f = torch.FloatTensor(test_f)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"9J5G-UbLVzGD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593606364120,"user_tz":-180,"elapsed":776,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"47468db4-e74d-4c01-fc9f-5e68bbb304ac"},"source":["train_f.shape"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2800, 2, 375, 5)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"UOJ-lEXYAUdG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1593606415116,"user_tz":-180,"elapsed":639,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"df92966a-d35d-4196-b642-f18bf1745385"},"source":["# make SKF groups\n","\n","def get_stratify_group(row):\n","    stratify_group = f'_{row[\"M\"]}'\n","    stratify_group += f'_{row[\"V\"]}'\n","    return stratify_group\n","\n","def get_labels(row):\n","    labels = f'{row[\"X\"]}'\n","    labels += f',{row[\"Y\"]}'\n","    labels += f',{row[\"M\"]}'\n","    labels += f',{row[\"V\"]}'\n","    return labels\n","\n","\n","\n","df_folds = train_t.copy()\n","df_folds = df_folds.sample(frac=1).reset_index(drop=True)\n","\n","df_folds['groups'] = df_folds.apply(get_stratify_group, axis=1)\n","df_folds['groups'] = df_folds['groups'].astype('category').cat.codes\n","\n","df_folds['labels'] = df_folds.apply(get_labels, axis=1)\n","\n","df_folds.head()"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>X</th>\n","      <th>Y</th>\n","      <th>M</th>\n","      <th>V</th>\n","      <th>groups</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2195</td>\n","      <td>0.0</td>\n","      <td>300.0</td>\n","      <td>125.0</td>\n","      <td>0.4</td>\n","      <td>6</td>\n","      <td>0.0,300.0,125.0,0.4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1994</td>\n","      <td>300.0</td>\n","      <td>200.0</td>\n","      <td>100.0</td>\n","      <td>0.6</td>\n","      <td>2</td>\n","      <td>300.0,200.0,100.0,0.6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1410</td>\n","      <td>300.0</td>\n","      <td>300.0</td>\n","      <td>150.0</td>\n","      <td>0.4</td>\n","      <td>11</td>\n","      <td>300.0,300.0,150.0,0.4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2648</td>\n","      <td>400.0</td>\n","      <td>200.0</td>\n","      <td>100.0</td>\n","      <td>0.6</td>\n","      <td>2</td>\n","      <td>400.0,200.0,100.0,0.6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1205</td>\n","      <td>400.0</td>\n","      <td>-300.0</td>\n","      <td>75.0</td>\n","      <td>0.4</td>\n","      <td>31</td>\n","      <td>400.0,-300.0,75.0,0.4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     id      X      Y      M    V  groups                 labels\n","0  2195    0.0  300.0  125.0  0.4       6    0.0,300.0,125.0,0.4\n","1  1994  300.0  200.0  100.0  0.6       2  300.0,200.0,100.0,0.6\n","2  1410  300.0  300.0  150.0  0.4      11  300.0,300.0,150.0,0.4\n","3  2648  400.0  200.0  100.0  0.6       2  400.0,200.0,100.0,0.6\n","4  1205  400.0 -300.0   75.0  0.4      31  400.0,-300.0,75.0,0.4"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"gPtiHg6-_nUt","colab_type":"code","colab":{}},"source":["df_folds.loc[:, 'fold'] = 0\n","\n","skf = stratified_group_k_fold(X=df_folds.index, \n","                              y=df_folds['groups'], \n","                              groups=df_folds['id'], k=5, seed=SEED)\n","\n","for fold_number, (train_index, val_index) in enumerate(tqdm(skf)):\n","    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bdyf9zK_nH1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1593606576694,"user_tz":-180,"elapsed":627,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"bfc600c6-d1a7-4213-be69-beb5c5ec9658"},"source":["df_folds.head()"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>X</th>\n","      <th>Y</th>\n","      <th>M</th>\n","      <th>V</th>\n","      <th>groups</th>\n","      <th>labels</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2195</td>\n","      <td>0.0</td>\n","      <td>300.0</td>\n","      <td>125.0</td>\n","      <td>0.4</td>\n","      <td>6</td>\n","      <td>0.0,300.0,125.0,0.4</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1994</td>\n","      <td>300.0</td>\n","      <td>200.0</td>\n","      <td>100.0</td>\n","      <td>0.6</td>\n","      <td>2</td>\n","      <td>300.0,200.0,100.0,0.6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1410</td>\n","      <td>300.0</td>\n","      <td>300.0</td>\n","      <td>150.0</td>\n","      <td>0.4</td>\n","      <td>11</td>\n","      <td>300.0,300.0,150.0,0.4</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2648</td>\n","      <td>400.0</td>\n","      <td>200.0</td>\n","      <td>100.0</td>\n","      <td>0.6</td>\n","      <td>2</td>\n","      <td>400.0,200.0,100.0,0.6</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1205</td>\n","      <td>400.0</td>\n","      <td>-300.0</td>\n","      <td>75.0</td>\n","      <td>0.4</td>\n","      <td>31</td>\n","      <td>400.0,-300.0,75.0,0.4</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     id      X      Y      M    V  groups                 labels  fold\n","0  2195    0.0  300.0  125.0  0.4       6    0.0,300.0,125.0,0.4     1\n","1  1994  300.0  200.0  100.0  0.6       2  300.0,200.0,100.0,0.6     0\n","2  1410  300.0  300.0  150.0  0.4      11  300.0,300.0,150.0,0.4     3\n","3  2648  400.0  200.0  100.0  0.6       2  400.0,200.0,100.0,0.6     1\n","4  1205  400.0 -300.0   75.0  0.4      31  400.0,-300.0,75.0,0.4     0"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"uiTBJLmEWr9a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593606632471,"user_tz":-180,"elapsed":605,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"58c89a3d-dd4a-43a2-a1c0-95e7e4886284"},"source":["df_folds.fold.nunique()"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"9L0zRxVTjMJD","colab_type":"code","colab":{}},"source":["df_folds.fold.nunique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jo7IjlqoiYFx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1593610195010,"user_tz":-180,"elapsed":598,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"c87e5d27-6782-4ece-b954-3c37f3d38e52"},"source":["for (train_idx, val_idx) in tqdm(skf):\n","        \n","        trainx, valx = train_f[train_idx], train_f[val_idx]\n","        trainy, valy = train_t[train_idx], train_t[val_idx]\n","\n","        print(trainx.shape, valx.shape)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["\n","0it [00:00, ?it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"FFjkpXdUkMVX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593610166878,"user_tz":-180,"elapsed":715,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"fee49f04-9b7a-4483-fe1a-b4beb9c297ad"},"source":["skf"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<generator object stratified_group_k_fold at 0x7f700732bfc0>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"PLr6PiRt2DX6","colab_type":"text"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"PMVh0PDn2DX7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593599918199,"user_tz":-180,"elapsed":699,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["def kfold_train(name, feature, target):\n","\n","    kf = KFold(nfold, shuffle=True, random_state=SEED)\n","    os.makedirs(save_path) if not os.path.exists(save_path) else None\n","    print('> {} train...'.format(name))\n","\n","    # make dataset\n","    n_features = feature.shape[-1]\n","    train_target = target[list(name)].values\n","    noise_add = Noise(0, 0.001, feature.shape[1:])\n","    loss_per_cv = []\n","    \n","    for i, (train_idx, val_idx) in enumerate(kf.split(feature, y = train_target)):\n","        \n","        print('-'*10)\n","        print('fold {}/{}'.format(i+1, nfold))\n","        print('-'*10)\n","\n","        trainx, valx= feature[train_idx], feature[val_idx]\n","        trainy, valy= train_target[train_idx], train_target[val_idx]\n","\n","        train_loader = DataLoader(\n","            dfDataset(trainx.astype(np.float32), trainy, transform=noise_add), batch_size=batch_size, shuffle=True)\n","        val_loader = DataLoader(\n","            dfDataset(valx.astype(np.float32), valy), batch_size=batch_size, shuffle=True)\n","        \n","        # model\n","        fe = custom_cnn()\n","        fc = custom_fc(input_dim=512*2*n_features, output_dim=len(name))\n","        model = nn.Sequential(fe, fc)\n","      \n","        optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n","\n","        if name == 'XY':\n","            criterion = E1_loss\n","        else:\n","            criterion = E2_loss\n","\n","        model = model.cuda()\n","        if initialize:\n","            model.apply(weights_init)\n","\n","        curr_loss = 1e+7\n","        \n","        # Train\n","        for ep in range(1, EPOCH + 1):\n","            loss, scores_train = train_model(model, train_loader, criterion, optimizer, criterion)\n","            val_loss, scores_val = eval_model(model, val_loader, criterion)\n","            \n","            if curr_loss > val_loss:\n","                print('[{}] train loss: {:4f}, train_score: {:.5f}, val loss drop: {:.4f} to: {:.4f}, val_score: {:.5f}'.format(ep, np.mean(loss), scores_train.avg, curr_loss, val_loss, scores_val.avg))\n","                \n","                curr_loss = val_loss\n","                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n","        loss_per_cv.append(curr_loss)\n","    return loss_per_cv           "],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"d5-7_p_Y2DX_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593602559981,"user_tz":-180,"elapsed":2605757,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"fb0720dd-ecd6-408f-b3e6-a58046062753"},"source":["# train XY\n","loss_xy = kfold_train('XY',train_f, train_t)\n","\n","add_feature = train_t[['X','Y']].values.reshape((2800, 1, 1, 2))\n","add_feature = np.repeat(add_feature, 375, axis = 2)\n","add_feature = np.repeat(add_feature, 2, axis = 1)\n","trainX = np.concatenate((train_f, add_feature), axis = -1)\n","\n","# train V using XY\n","loss_v = kfold_train('V',trainX, train_t)\n","\n","add_feature = train_t[['V']].values.reshape((2800, 1, 1, 1))\n","add_feature = np.repeat(add_feature, 375, axis = 2)\n","add_feature = np.repeat(add_feature, 2, axis = 1)\n","trainX = np.concatenate((trainX, add_feature), axis = -1)\n","\n","# train M using.. \n","loss_m = kfold_train('M',trainX, train_t)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["> XY train...\n","----------\n","fold 1/5\n","----------\n","[1] : train loss: 2.722458, val loss drop: 10000000.0000 to: 1.5495\n","[2] : train loss: 0.781087, val loss drop: 1.5495 to: 0.3483\n","[3] : train loss: 0.239417, val loss drop: 0.3483 to: 0.1666\n","[4] : train loss: 0.089852, val loss drop: 0.1666 to: 0.0969\n","[5] : train loss: 0.039067, val loss drop: 0.0969 to: 0.0477\n","[6] : train loss: 0.024801, val loss drop: 0.0477 to: 0.0429\n","[7] : train loss: 0.013968, val loss drop: 0.0429 to: 0.0125\n","[8] : train loss: 0.009092, val loss drop: 0.0125 to: 0.0100\n","[9] : train loss: 0.007064, val loss drop: 0.0100 to: 0.0089\n","[10] : train loss: 0.004731, val loss drop: 0.0089 to: 0.0054\n","[11] : train loss: 0.003910, val loss drop: 0.0054 to: 0.0050\n","[12] : train loss: 0.002779, val loss drop: 0.0050 to: 0.0039\n","[14] : train loss: 0.002637, val loss drop: 0.0039 to: 0.0035\n","[17] : train loss: 0.003059, val loss drop: 0.0035 to: 0.0034\n","[18] : train loss: 0.002204, val loss drop: 0.0034 to: 0.0032\n","[19] : train loss: 0.001889, val loss drop: 0.0032 to: 0.0028\n","[22] : train loss: 0.002157, val loss drop: 0.0028 to: 0.0023\n","[23] : train loss: 0.001630, val loss drop: 0.0023 to: 0.0021\n","[26] : train loss: 0.001617, val loss drop: 0.0021 to: 0.0020\n","[27] : train loss: 0.001193, val loss drop: 0.0020 to: 0.0018\n","[36] : train loss: 0.000970, val loss drop: 0.0018 to: 0.0016\n","[41] : train loss: 0.000863, val loss drop: 0.0016 to: 0.0015\n","[44] : train loss: 0.001150, val loss drop: 0.0015 to: 0.0013\n","[51] : train loss: 0.001043, val loss drop: 0.0013 to: 0.0011\n","[77] : train loss: 0.001054, val loss drop: 0.0011 to: 0.0011\n","[80] : train loss: 0.001696, val loss drop: 0.0011 to: 0.0010\n","[84] : train loss: 0.000644, val loss drop: 0.0010 to: 0.0010\n","[85] : train loss: 0.000423, val loss drop: 0.0010 to: 0.0008\n","[105] : train loss: 0.000459, val loss drop: 0.0008 to: 0.0007\n","[134] : train loss: 0.000582, val loss drop: 0.0007 to: 0.0007\n","[141] : train loss: 0.000386, val loss drop: 0.0007 to: 0.0006\n","[156] : train loss: 0.000620, val loss drop: 0.0006 to: 0.0006\n","[190] : train loss: 0.000481, val loss drop: 0.0006 to: 0.0005\n","----------\n","fold 2/5\n","----------\n","[1] : train loss: 2.750280, val loss drop: 10000000.0000 to: 1.5862\n","[2] : train loss: 0.816994, val loss drop: 1.5862 to: 0.2758\n","[3] : train loss: 0.211088, val loss drop: 0.2758 to: 0.1576\n","[4] : train loss: 0.082200, val loss drop: 0.1576 to: 0.1041\n","[5] : train loss: 0.038405, val loss drop: 0.1041 to: 0.0426\n","[6] : train loss: 0.018824, val loss drop: 0.0426 to: 0.0244\n","[7] : train loss: 0.011764, val loss drop: 0.0244 to: 0.0140\n","[8] : train loss: 0.007331, val loss drop: 0.0140 to: 0.0127\n","[9] : train loss: 0.006042, val loss drop: 0.0127 to: 0.0076\n","[10] : train loss: 0.004648, val loss drop: 0.0076 to: 0.0056\n","[12] : train loss: 0.003008, val loss drop: 0.0056 to: 0.0030\n","[15] : train loss: 0.002168, val loss drop: 0.0030 to: 0.0028\n","[17] : train loss: 0.001647, val loss drop: 0.0028 to: 0.0023\n","[19] : train loss: 0.001905, val loss drop: 0.0023 to: 0.0021\n","[20] : train loss: 0.002092, val loss drop: 0.0021 to: 0.0020\n","[23] : train loss: 0.001601, val loss drop: 0.0020 to: 0.0020\n","[24] : train loss: 0.000937, val loss drop: 0.0020 to: 0.0012\n","[26] : train loss: 0.000833, val loss drop: 0.0012 to: 0.0012\n","[33] : train loss: 0.001076, val loss drop: 0.0012 to: 0.0011\n","[35] : train loss: 0.000702, val loss drop: 0.0011 to: 0.0009\n","[43] : train loss: 0.000653, val loss drop: 0.0009 to: 0.0009\n","[52] : train loss: 0.000588, val loss drop: 0.0009 to: 0.0008\n","[65] : train loss: 0.000587, val loss drop: 0.0008 to: 0.0006\n","[98] : train loss: 0.000455, val loss drop: 0.0006 to: 0.0006\n","[99] : train loss: 0.000362, val loss drop: 0.0006 to: 0.0006\n","[100] : train loss: 0.000232, val loss drop: 0.0006 to: 0.0004\n","[191] : train loss: 0.000457, val loss drop: 0.0004 to: 0.0003\n","----------\n","fold 3/5\n","----------\n","[1] : train loss: 2.685616, val loss drop: 10000000.0000 to: 1.7852\n","[2] : train loss: 0.725234, val loss drop: 1.7852 to: 0.3421\n","[3] : train loss: 0.178177, val loss drop: 0.3421 to: 0.1660\n","[4] : train loss: 0.067169, val loss drop: 0.1660 to: 0.1142\n","[5] : train loss: 0.028667, val loss drop: 0.1142 to: 0.0375\n","[6] : train loss: 0.015683, val loss drop: 0.0375 to: 0.0328\n","[7] : train loss: 0.010495, val loss drop: 0.0328 to: 0.0102\n","[8] : train loss: 0.007777, val loss drop: 0.0102 to: 0.0085\n","[9] : train loss: 0.006601, val loss drop: 0.0085 to: 0.0064\n","[11] : train loss: 0.003339, val loss drop: 0.0064 to: 0.0042\n","[12] : train loss: 0.002626, val loss drop: 0.0042 to: 0.0037\n","[13] : train loss: 0.002089, val loss drop: 0.0037 to: 0.0035\n","[14] : train loss: 0.002031, val loss drop: 0.0035 to: 0.0034\n","[16] : train loss: 0.002955, val loss drop: 0.0034 to: 0.0030\n","[19] : train loss: 0.002390, val loss drop: 0.0030 to: 0.0028\n","[20] : train loss: 0.002083, val loss drop: 0.0028 to: 0.0021\n","[25] : train loss: 0.002456, val loss drop: 0.0021 to: 0.0020\n","[26] : train loss: 0.001328, val loss drop: 0.0020 to: 0.0018\n","[27] : train loss: 0.001292, val loss drop: 0.0018 to: 0.0016\n","[32] : train loss: 0.001182, val loss drop: 0.0016 to: 0.0016\n","[33] : train loss: 0.002157, val loss drop: 0.0016 to: 0.0015\n","[37] : train loss: 0.001585, val loss drop: 0.0015 to: 0.0013\n","[43] : train loss: 0.000755, val loss drop: 0.0013 to: 0.0009\n","[44] : train loss: 0.000423, val loss drop: 0.0009 to: 0.0009\n","[45] : train loss: 0.000407, val loss drop: 0.0009 to: 0.0009\n","[62] : train loss: 0.000504, val loss drop: 0.0009 to: 0.0007\n","[63] : train loss: 0.000438, val loss drop: 0.0007 to: 0.0005\n","[102] : train loss: 0.000759, val loss drop: 0.0005 to: 0.0005\n","[104] : train loss: 0.000326, val loss drop: 0.0005 to: 0.0005\n","[132] : train loss: 0.000445, val loss drop: 0.0005 to: 0.0004\n","[139] : train loss: 0.000436, val loss drop: 0.0004 to: 0.0004\n","[175] : train loss: 0.000350, val loss drop: 0.0004 to: 0.0004\n","[182] : train loss: 0.000353, val loss drop: 0.0004 to: 0.0003\n","----------\n","fold 4/5\n","----------\n","[1] : train loss: 2.844829, val loss drop: 10000000.0000 to: 1.6509\n","[2] : train loss: 0.900318, val loss drop: 1.6509 to: 0.2893\n","[3] : train loss: 0.228999, val loss drop: 0.2893 to: 0.1888\n","[4] : train loss: 0.091176, val loss drop: 0.1888 to: 0.0832\n","[5] : train loss: 0.041025, val loss drop: 0.0832 to: 0.0354\n","[7] : train loss: 0.012811, val loss drop: 0.0354 to: 0.0119\n","[8] : train loss: 0.009924, val loss drop: 0.0119 to: 0.0107\n","[9] : train loss: 0.006206, val loss drop: 0.0107 to: 0.0065\n","[10] : train loss: 0.004445, val loss drop: 0.0065 to: 0.0065\n","[11] : train loss: 0.004050, val loss drop: 0.0065 to: 0.0044\n","[12] : train loss: 0.003200, val loss drop: 0.0044 to: 0.0039\n","[13] : train loss: 0.002526, val loss drop: 0.0039 to: 0.0032\n","[14] : train loss: 0.002215, val loss drop: 0.0032 to: 0.0027\n","[16] : train loss: 0.001994, val loss drop: 0.0027 to: 0.0026\n","[18] : train loss: 0.001771, val loss drop: 0.0026 to: 0.0023\n","[21] : train loss: 0.001529, val loss drop: 0.0023 to: 0.0022\n","[22] : train loss: 0.001086, val loss drop: 0.0022 to: 0.0018\n","[24] : train loss: 0.001106, val loss drop: 0.0018 to: 0.0017\n","[26] : train loss: 0.001153, val loss drop: 0.0017 to: 0.0016\n","[30] : train loss: 0.001195, val loss drop: 0.0016 to: 0.0016\n","[32] : train loss: 0.000934, val loss drop: 0.0016 to: 0.0014\n","[33] : train loss: 0.000947, val loss drop: 0.0014 to: 0.0013\n","[34] : train loss: 0.000527, val loss drop: 0.0013 to: 0.0012\n","[41] : train loss: 0.000633, val loss drop: 0.0012 to: 0.0011\n","[43] : train loss: 0.000540, val loss drop: 0.0011 to: 0.0010\n","[54] : train loss: 0.001107, val loss drop: 0.0010 to: 0.0009\n","[71] : train loss: 0.000569, val loss drop: 0.0009 to: 0.0006\n","[98] : train loss: 0.000340, val loss drop: 0.0006 to: 0.0006\n","[100] : train loss: 0.000376, val loss drop: 0.0006 to: 0.0005\n","[157] : train loss: 0.000187, val loss drop: 0.0005 to: 0.0004\n","[176] : train loss: 0.000284, val loss drop: 0.0004 to: 0.0004\n","----------\n","fold 5/5\n","----------\n","[1] : train loss: 2.804048, val loss drop: 10000000.0000 to: 1.8279\n","[2] : train loss: 0.858944, val loss drop: 1.8279 to: 0.2901\n","[3] : train loss: 0.221731, val loss drop: 0.2901 to: 0.1546\n","[4] : train loss: 0.072197, val loss drop: 0.1546 to: 0.0604\n","[5] : train loss: 0.027966, val loss drop: 0.0604 to: 0.0414\n","[6] : train loss: 0.017283, val loss drop: 0.0414 to: 0.0260\n","[7] : train loss: 0.010165, val loss drop: 0.0260 to: 0.0129\n","[8] : train loss: 0.006653, val loss drop: 0.0129 to: 0.0105\n","[9] : train loss: 0.004590, val loss drop: 0.0105 to: 0.0075\n","[10] : train loss: 0.003681, val loss drop: 0.0075 to: 0.0060\n","[11] : train loss: 0.003015, val loss drop: 0.0060 to: 0.0052\n","[13] : train loss: 0.002621, val loss drop: 0.0052 to: 0.0036\n","[14] : train loss: 0.002276, val loss drop: 0.0036 to: 0.0036\n","[15] : train loss: 0.002045, val loss drop: 0.0036 to: 0.0034\n","[17] : train loss: 0.001945, val loss drop: 0.0034 to: 0.0029\n","[18] : train loss: 0.001377, val loss drop: 0.0029 to: 0.0023\n","[19] : train loss: 0.001139, val loss drop: 0.0023 to: 0.0022\n","[20] : train loss: 0.001050, val loss drop: 0.0022 to: 0.0021\n","[25] : train loss: 0.001339, val loss drop: 0.0021 to: 0.0020\n","[26] : train loss: 0.001295, val loss drop: 0.0020 to: 0.0020\n","[27] : train loss: 0.001148, val loss drop: 0.0020 to: 0.0020\n","[28] : train loss: 0.000769, val loss drop: 0.0020 to: 0.0015\n","[52] : train loss: 0.001535, val loss drop: 0.0015 to: 0.0011\n","[60] : train loss: 0.000427, val loss drop: 0.0011 to: 0.0009\n","[68] : train loss: 0.000383, val loss drop: 0.0009 to: 0.0009\n","[83] : train loss: 0.000386, val loss drop: 0.0009 to: 0.0008\n","[84] : train loss: 0.000396, val loss drop: 0.0008 to: 0.0008\n","[97] : train loss: 0.000611, val loss drop: 0.0008 to: 0.0007\n","[118] : train loss: 0.000408, val loss drop: 0.0007 to: 0.0007\n","[133] : train loss: 0.000235, val loss drop: 0.0007 to: 0.0006\n","[166] : train loss: 0.000494, val loss drop: 0.0006 to: 0.0006\n","[183] : train loss: 0.000395, val loss drop: 0.0006 to: 0.0005\n","[192] : train loss: 0.000281, val loss drop: 0.0005 to: 0.0005\n","> V train...\n","----------\n","fold 1/5\n","----------\n","[1] : train loss: 25.054720, val loss drop: 10000000.0000 to: 0.2083\n","[2] : train loss: 0.205565, val loss drop: 0.2083 to: 0.0221\n","[5] : train loss: 0.019519, val loss drop: 0.0221 to: 0.0142\n","[9] : train loss: 0.017157, val loss drop: 0.0142 to: 0.0072\n","[10] : train loss: 0.013536, val loss drop: 0.0072 to: 0.0047\n","[14] : train loss: 0.013654, val loss drop: 0.0047 to: 0.0041\n","[20] : train loss: 0.020068, val loss drop: 0.0041 to: 0.0031\n","[23] : train loss: 0.004184, val loss drop: 0.0031 to: 0.0029\n","[24] : train loss: 0.006152, val loss drop: 0.0029 to: 0.0027\n","[31] : train loss: 0.004547, val loss drop: 0.0027 to: 0.0022\n","[42] : train loss: 0.005590, val loss drop: 0.0022 to: 0.0019\n","[48] : train loss: 0.008237, val loss drop: 0.0019 to: 0.0018\n","[53] : train loss: 0.011866, val loss drop: 0.0018 to: 0.0015\n","[58] : train loss: 0.003228, val loss drop: 0.0015 to: 0.0013\n","[89] : train loss: 0.008868, val loss drop: 0.0013 to: 0.0010\n","[91] : train loss: 0.002321, val loss drop: 0.0010 to: 0.0008\n","[108] : train loss: 0.000891, val loss drop: 0.0008 to: 0.0006\n","[166] : train loss: 0.007815, val loss drop: 0.0006 to: 0.0006\n","[172] : train loss: 0.001200, val loss drop: 0.0006 to: 0.0004\n","[195] : train loss: 0.003837, val loss drop: 0.0004 to: 0.0004\n","----------\n","fold 2/5\n","----------\n","[1] : train loss: 28.226028, val loss drop: 10000000.0000 to: 4.1384\n","[2] : train loss: 1.128204, val loss drop: 4.1384 to: 1.7492\n","[3] : train loss: 0.442165, val loss drop: 1.7492 to: 0.2655\n","[4] : train loss: 0.201235, val loss drop: 0.2655 to: 0.0495\n","[6] : train loss: 0.038125, val loss drop: 0.0495 to: 0.0271\n","[7] : train loss: 0.016822, val loss drop: 0.0271 to: 0.0167\n","[8] : train loss: 0.009869, val loss drop: 0.0167 to: 0.0104\n","[9] : train loss: 0.009572, val loss drop: 0.0104 to: 0.0079\n","[12] : train loss: 0.006626, val loss drop: 0.0079 to: 0.0076\n","[13] : train loss: 0.007644, val loss drop: 0.0076 to: 0.0067\n","[20] : train loss: 0.008081, val loss drop: 0.0067 to: 0.0044\n","[23] : train loss: 0.003935, val loss drop: 0.0044 to: 0.0043\n","[25] : train loss: 0.003636, val loss drop: 0.0043 to: 0.0041\n","[27] : train loss: 0.004991, val loss drop: 0.0041 to: 0.0034\n","[33] : train loss: 0.006950, val loss drop: 0.0034 to: 0.0027\n","[36] : train loss: 0.007935, val loss drop: 0.0027 to: 0.0024\n","[51] : train loss: 0.002837, val loss drop: 0.0024 to: 0.0017\n","[57] : train loss: 0.001534, val loss drop: 0.0017 to: 0.0016\n","[91] : train loss: 0.012474, val loss drop: 0.0016 to: 0.0012\n","[141] : train loss: 0.002784, val loss drop: 0.0012 to: 0.0008\n","[197] : train loss: 0.002142, val loss drop: 0.0008 to: 0.0007\n","----------\n","fold 3/5\n","----------\n","[1] : train loss: 31.586610, val loss drop: 10000000.0000 to: 1.5105\n","[2] : train loss: 0.459259, val loss drop: 1.5105 to: 0.7614\n","[3] : train loss: 0.191586, val loss drop: 0.7614 to: 0.0265\n","[5] : train loss: 0.062244, val loss drop: 0.0265 to: 0.0121\n","[9] : train loss: 0.012552, val loss drop: 0.0121 to: 0.0048\n","[13] : train loss: 0.008391, val loss drop: 0.0048 to: 0.0040\n","[15] : train loss: 0.004745, val loss drop: 0.0040 to: 0.0039\n","[19] : train loss: 0.011033, val loss drop: 0.0039 to: 0.0029\n","[23] : train loss: 0.003574, val loss drop: 0.0029 to: 0.0027\n","[36] : train loss: 0.006590, val loss drop: 0.0027 to: 0.0025\n","[40] : train loss: 0.003025, val loss drop: 0.0025 to: 0.0018\n","[60] : train loss: 0.011813, val loss drop: 0.0018 to: 0.0015\n","[101] : train loss: 0.002844, val loss drop: 0.0015 to: 0.0009\n","[102] : train loss: 0.002170, val loss drop: 0.0009 to: 0.0009\n","[116] : train loss: 0.002407, val loss drop: 0.0009 to: 0.0007\n","[182] : train loss: 0.001877, val loss drop: 0.0007 to: 0.0007\n","[183] : train loss: 0.003215, val loss drop: 0.0007 to: 0.0004\n","----------\n","fold 4/5\n","----------\n","[1] : train loss: 15.371551, val loss drop: 10000000.0000 to: 5.3386\n","[2] : train loss: 0.475640, val loss drop: 5.3386 to: 0.5738\n","[3] : train loss: 0.198441, val loss drop: 0.5738 to: 0.0494\n","[4] : train loss: 0.075206, val loss drop: 0.0494 to: 0.0283\n","[5] : train loss: 0.034947, val loss drop: 0.0283 to: 0.0142\n","[6] : train loss: 0.015329, val loss drop: 0.0142 to: 0.0106\n","[7] : train loss: 0.009381, val loss drop: 0.0106 to: 0.0089\n","[8] : train loss: 0.006003, val loss drop: 0.0089 to: 0.0075\n","[9] : train loss: 0.005061, val loss drop: 0.0075 to: 0.0068\n","[10] : train loss: 0.004760, val loss drop: 0.0068 to: 0.0064\n","[11] : train loss: 0.004268, val loss drop: 0.0064 to: 0.0055\n","[12] : train loss: 0.008443, val loss drop: 0.0055 to: 0.0049\n","[14] : train loss: 0.005815, val loss drop: 0.0049 to: 0.0045\n","[19] : train loss: 0.004669, val loss drop: 0.0045 to: 0.0034\n","[20] : train loss: 0.003279, val loss drop: 0.0034 to: 0.0033\n","[21] : train loss: 0.002412, val loss drop: 0.0033 to: 0.0031\n","[29] : train loss: 0.006942, val loss drop: 0.0031 to: 0.0025\n","[39] : train loss: 0.002385, val loss drop: 0.0025 to: 0.0017\n","[65] : train loss: 0.008238, val loss drop: 0.0017 to: 0.0015\n","[67] : train loss: 0.009976, val loss drop: 0.0015 to: 0.0015\n","[73] : train loss: 0.003167, val loss drop: 0.0015 to: 0.0014\n","[78] : train loss: 0.002061, val loss drop: 0.0014 to: 0.0014\n","[79] : train loss: 0.003547, val loss drop: 0.0014 to: 0.0012\n","[126] : train loss: 0.002888, val loss drop: 0.0012 to: 0.0011\n","[141] : train loss: 0.002294, val loss drop: 0.0011 to: 0.0011\n","[146] : train loss: 0.001407, val loss drop: 0.0011 to: 0.0010\n","[149] : train loss: 0.003566, val loss drop: 0.0010 to: 0.0008\n","----------\n","fold 5/5\n","----------\n","[1] : train loss: 10.761900, val loss drop: 10000000.0000 to: 0.7600\n","[2] : train loss: 0.176787, val loss drop: 0.7600 to: 0.0360\n","[3] : train loss: 0.064672, val loss drop: 0.0360 to: 0.0317\n","[5] : train loss: 0.019629, val loss drop: 0.0317 to: 0.0187\n","[6] : train loss: 0.010000, val loss drop: 0.0187 to: 0.0061\n","[7] : train loss: 0.005437, val loss drop: 0.0061 to: 0.0057\n","[9] : train loss: 0.009623, val loss drop: 0.0057 to: 0.0040\n","[12] : train loss: 0.002754, val loss drop: 0.0040 to: 0.0026\n","[23] : train loss: 0.012515, val loss drop: 0.0026 to: 0.0020\n","[31] : train loss: 0.008211, val loss drop: 0.0020 to: 0.0015\n","[53] : train loss: 0.001561, val loss drop: 0.0015 to: 0.0014\n","[58] : train loss: 0.003251, val loss drop: 0.0014 to: 0.0011\n","[64] : train loss: 0.004443, val loss drop: 0.0011 to: 0.0011\n","[82] : train loss: 0.002473, val loss drop: 0.0011 to: 0.0009\n","[83] : train loss: 0.000700, val loss drop: 0.0009 to: 0.0007\n","[108] : train loss: 0.001396, val loss drop: 0.0007 to: 0.0007\n","[109] : train loss: 0.002744, val loss drop: 0.0007 to: 0.0006\n","[154] : train loss: 0.002877, val loss drop: 0.0006 to: 0.0004\n","> M train...\n","----------\n","fold 1/5\n","----------\n","[1] : train loss: 31.806573, val loss drop: 10000000.0000 to: 61.0656\n","[2] : train loss: 6.623850, val loss drop: 61.0656 to: 39.9217\n","[3] : train loss: 2.743062, val loss drop: 39.9217 to: 2.5982\n","[4] : train loss: 1.483489, val loss drop: 2.5982 to: 1.0129\n","[6] : train loss: 0.515725, val loss drop: 1.0129 to: 0.4041\n","[9] : train loss: 0.482115, val loss drop: 0.4041 to: 0.2553\n","[11] : train loss: 0.412721, val loss drop: 0.2553 to: 0.2505\n","[12] : train loss: 0.284608, val loss drop: 0.2505 to: 0.1693\n","[13] : train loss: 0.391303, val loss drop: 0.1693 to: 0.1544\n","[21] : train loss: 0.186613, val loss drop: 0.1544 to: 0.1128\n","[26] : train loss: 0.232141, val loss drop: 0.1128 to: 0.0468\n","[48] : train loss: 0.177920, val loss drop: 0.0468 to: 0.0388\n","[67] : train loss: 0.098005, val loss drop: 0.0388 to: 0.0351\n","[70] : train loss: 0.224044, val loss drop: 0.0351 to: 0.0233\n","[156] : train loss: 0.065796, val loss drop: 0.0233 to: 0.0168\n","[186] : train loss: 0.319938, val loss drop: 0.0168 to: 0.0145\n","----------\n","fold 2/5\n","----------\n","[1] : train loss: 34.490083, val loss drop: 10000000.0000 to: 8.5020\n","[2] : train loss: 6.143755, val loss drop: 8.5020 to: 2.3252\n","[3] : train loss: 2.429775, val loss drop: 2.3252 to: 2.1445\n","[4] : train loss: 1.126864, val loss drop: 2.1445 to: 1.3721\n","[6] : train loss: 0.763033, val loss drop: 1.3721 to: 0.4468\n","[7] : train loss: 0.365683, val loss drop: 0.4468 to: 0.3142\n","[8] : train loss: 0.384252, val loss drop: 0.3142 to: 0.2712\n","[11] : train loss: 0.749014, val loss drop: 0.2712 to: 0.2662\n","[12] : train loss: 0.258513, val loss drop: 0.2662 to: 0.1817\n","[15] : train loss: 0.270647, val loss drop: 0.1817 to: 0.1252\n","[19] : train loss: 0.457514, val loss drop: 0.1252 to: 0.1067\n","[21] : train loss: 0.301447, val loss drop: 0.1067 to: 0.1066\n","[23] : train loss: 0.157087, val loss drop: 0.1066 to: 0.0874\n","[25] : train loss: 0.258439, val loss drop: 0.0874 to: 0.0822\n","[29] : train loss: 0.463779, val loss drop: 0.0822 to: 0.0785\n","[30] : train loss: 0.149972, val loss drop: 0.0785 to: 0.0605\n","[32] : train loss: 0.236252, val loss drop: 0.0605 to: 0.0507\n","[40] : train loss: 0.208088, val loss drop: 0.0507 to: 0.0422\n","[66] : train loss: 0.471098, val loss drop: 0.0422 to: 0.0380\n","[82] : train loss: 0.203422, val loss drop: 0.0380 to: 0.0371\n","[99] : train loss: 0.269795, val loss drop: 0.0371 to: 0.0238\n","[112] : train loss: 0.108555, val loss drop: 0.0238 to: 0.0174\n","[126] : train loss: 0.082870, val loss drop: 0.0174 to: 0.0155\n","----------\n","fold 3/5\n","----------\n","[1] : train loss: 33.402580, val loss drop: 10000000.0000 to: 17.0972\n","[2] : train loss: 5.634955, val loss drop: 17.0972 to: 5.5596\n","[5] : train loss: 0.613444, val loss drop: 5.5596 to: 0.6324\n","[6] : train loss: 0.482550, val loss drop: 0.6324 to: 0.3792\n","[9] : train loss: 0.331282, val loss drop: 0.3792 to: 0.3407\n","[10] : train loss: 0.317149, val loss drop: 0.3407 to: 0.2421\n","[12] : train loss: 0.399597, val loss drop: 0.2421 to: 0.2053\n","[23] : train loss: 0.169070, val loss drop: 0.2053 to: 0.1688\n","[30] : train loss: 0.400962, val loss drop: 0.1688 to: 0.1352\n","[31] : train loss: 0.238204, val loss drop: 0.1352 to: 0.0872\n","[56] : train loss: 0.158579, val loss drop: 0.0872 to: 0.0731\n","[69] : train loss: 0.306751, val loss drop: 0.0731 to: 0.0569\n","[70] : train loss: 0.129346, val loss drop: 0.0569 to: 0.0361\n","[101] : train loss: 0.117489, val loss drop: 0.0361 to: 0.0348\n","[110] : train loss: 0.219850, val loss drop: 0.0348 to: 0.0266\n","[119] : train loss: 0.089701, val loss drop: 0.0266 to: 0.0251\n","----------\n","fold 4/5\n","----------\n","[1] : train loss: 41.552424, val loss drop: 10000000.0000 to: 15.1092\n","[2] : train loss: 6.230052, val loss drop: 15.1092 to: 5.1897\n","[3] : train loss: 2.258694, val loss drop: 5.1897 to: 5.1625\n","[4] : train loss: 1.017328, val loss drop: 5.1625 to: 1.7416\n","[7] : train loss: 0.465324, val loss drop: 1.7416 to: 0.8554\n","[8] : train loss: 0.352297, val loss drop: 0.8554 to: 0.7168\n","[9] : train loss: 0.384935, val loss drop: 0.7168 to: 0.2209\n","[12] : train loss: 0.262181, val loss drop: 0.2209 to: 0.1339\n","[22] : train loss: 0.263994, val loss drop: 0.1339 to: 0.0961\n","[32] : train loss: 0.189224, val loss drop: 0.0961 to: 0.0538\n","[39] : train loss: 0.136946, val loss drop: 0.0538 to: 0.0413\n","[41] : train loss: 0.087297, val loss drop: 0.0413 to: 0.0379\n","[65] : train loss: 0.112734, val loss drop: 0.0379 to: 0.0295\n","[66] : train loss: 0.085060, val loss drop: 0.0295 to: 0.0249\n","[85] : train loss: 0.300804, val loss drop: 0.0249 to: 0.0246\n","[95] : train loss: 0.114535, val loss drop: 0.0246 to: 0.0231\n","[99] : train loss: 0.135714, val loss drop: 0.0231 to: 0.0146\n","[138] : train loss: 0.182005, val loss drop: 0.0146 to: 0.0138\n","[143] : train loss: 0.096313, val loss drop: 0.0138 to: 0.0116\n","----------\n","fold 5/5\n","----------\n","[1] : train loss: 30.240111, val loss drop: 10000000.0000 to: 32.1954\n","[2] : train loss: 5.000488, val loss drop: 32.1954 to: 5.0905\n","[3] : train loss: 1.543215, val loss drop: 5.0905 to: 1.2021\n","[5] : train loss: 0.885367, val loss drop: 1.2021 to: 1.1539\n","[6] : train loss: 0.530041, val loss drop: 1.1539 to: 0.7656\n","[7] : train loss: 0.453129, val loss drop: 0.7656 to: 0.2879\n","[15] : train loss: 0.391104, val loss drop: 0.2879 to: 0.2829\n","[16] : train loss: 0.248110, val loss drop: 0.2829 to: 0.1148\n","[21] : train loss: 0.200735, val loss drop: 0.1148 to: 0.0733\n","[28] : train loss: 0.214063, val loss drop: 0.0733 to: 0.0675\n","[43] : train loss: 0.166158, val loss drop: 0.0675 to: 0.0528\n","[56] : train loss: 0.144639, val loss drop: 0.0528 to: 0.0364\n","[61] : train loss: 0.458607, val loss drop: 0.0364 to: 0.0310\n","[85] : train loss: 0.096972, val loss drop: 0.0310 to: 0.0293\n","[96] : train loss: 0.252264, val loss drop: 0.0293 to: 0.0285\n","[101] : train loss: 0.306054, val loss drop: 0.0285 to: 0.0276\n","[119] : train loss: 0.349733, val loss drop: 0.0276 to: 0.0227\n","[138] : train loss: 0.116281, val loss drop: 0.0227 to: 0.0182\n","[171] : train loss: 0.053535, val loss drop: 0.0182 to: 0.0127\n","[174] : train loss: 0.106790, val loss drop: 0.0127 to: 0.0109\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V3KNWsOP2DYE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593602860154,"user_tz":-180,"elapsed":893,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["loss_per_model = {'xy':loss_xy, 'v':loss_v, 'm':loss_m}\n","with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n","    for k in loss_per_model:\n","        loss_per_model[k] = np.mean(loss_per_model[k])\n","    f.write(json.dumps(loss_per_model))"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"c47JzCv_If9z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593602879884,"user_tz":-180,"elapsed":648,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"2d4a9152-38bc-40ef-ed05-e34cd5c4c197"},"source":["loss_per_model"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'m': 0.015528180976863018,\n"," 'v': 0.0005324836457697251,\n"," 'xy': 0.0003994177532988942}"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"u_sT-ERc2DYJ","colab_type":"code","colab":{},"outputId":"3487fb8c-62fb-4479-b048-e0bb6b2dddeb"},"source":["loss_per_model # leaky relu"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'xy': 0.000625143100911778,\n"," 'v': 0.0010803749855117096,\n"," 'm': 0.02109942916598308}"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"2S6ol5K42DYM","colab_type":"code","colab":{},"outputId":"cd21fa34-39b7-40dc-94ef-15d374f185be"},"source":["loss_per_model #elu"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'xy': 0.000880345263100234,\n"," 'v': 0.0009727095718393787,\n"," 'm': 0.01579595682751895}"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"tQRMbbTB2DYQ","colab_type":"code","colab":{},"outputId":"a4b9d09d-8c9f-4698-b3cb-caf94966f19e"},"source":["loss_per_model # relu"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'xy': 0.0006840781165308819,\n"," 'v': 0.0010359705444652266,\n"," 'm': 0.023326632654456333}"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"RxGxKTsJ2DYT","colab_type":"text"},"source":["- xy : leakyrelu > relu > elu\n","- v : elu > relu > leakyrelu\n","- m : elu > leakyrelu > relu"]},{"cell_type":"markdown","metadata":{"id":"pNR0Q3zM2DYU","colab_type":"text"},"source":["# Predict Test set"]},{"cell_type":"code","metadata":{"id":"WGATI6cv2DYV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593603016052,"user_tz":-180,"elapsed":654,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["def predict_fold(model,nfold, save_path, name, test_data):\n","    pred_array = []\n","    for i in range(1, nfold+1):\n","        model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n","        model = model.cuda()\n","        \n","        with torch.no_grad():\n","            predict = model(test_data.cuda())\n","        pred_array.append(predict.detach().cpu().numpy())\n","    result = np.mean(pred_array, axis = 0)\n","    return result"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1EZtS7i95mW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1593603019029,"user_tz":-180,"elapsed":1027,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"3b28c11a-4e46-4c7b-8282-aa65d8ed9e65"},"source":["submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n","submission.head()"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>X</th>\n","      <th>Y</th>\n","      <th>M</th>\n","      <th>V</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2800</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2801</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2802</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2803</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2804</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     id  X  Y  M  V\n","0  2800  0  0  0  0\n","1  2801  0  0  0  0\n","2  2802  0  0  0  0\n","3  2803  0  0  0  0\n","4  2804  0  0  0  0"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"1xZjsJ2W2DYY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593603078456,"user_tz":-180,"elapsed":902,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["# predict XY\n","name = 'XY'\n","n_features = test_f.size()[-1]\n","\n","# define model\n","fe = custom_cnn()\n","fc = custom_fc(512*2*n_features, len(name))\n","model = nn.Sequential(fe, fc)\n","\n","result = predict_fold(model, nfold, save_path ,name, test_f)\n","submission[list(name)] = result"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"FIh6nv6T2DYc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593603087518,"user_tz":-180,"elapsed":763,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["n_samples = test_f.shape[0]\n","add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n","add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n","add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n","add_feature_t = torch.FloatTensor(add_feature_t)\n","\n","test_f_add = torch.cat([test_f, add_feature_t], dim = -1)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"AebWXDsh2DYg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593603118440,"user_tz":-180,"elapsed":641,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["# predict V\n","name = 'V'\n","n_features = test_f_add.size()[-1]\n","# define model\n","fe = custom_cnn()\n","fc = custom_fc(512*2*n_features, len(name))\n","model = nn.Sequential(fe, fc)\n","\n","result = predict_fold(model, nfold, save_path,name, test_f_add)\n","submission[list(name)] = result"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"05kF5j4j2DYj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593603124888,"user_tz":-180,"elapsed":585,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["n_samples = test_f_add.shape[0]\n","add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n","add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n","add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n","add_feature_t = torch.FloatTensor(add_feature_t)\n","\n","test_f_add = torch.cat([test_f_add, add_feature_t], dim = -1)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"IbS5Kfds2DYm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593603131738,"user_tz":-180,"elapsed":904,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["# predict M\n","name = 'M'\n","n_features = test_f_add.size()[-1]\n","# define model\n","\n","fe = custom_cnn()\n","fc = custom_fc(512*2*n_features, len(name))\n","model = nn.Sequential(fe, fc)\n","\n","result = predict_fold(model, nfold, save_path,name, test_f_add)\n","submission[list(name)] = result"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-vCr7f52DYq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1593603134322,"user_tz":-180,"elapsed":594,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"178aaeda-59ca-49ea-a493-1b2c81194427"},"source":["submission.head()"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>X</th>\n","      <th>Y</th>\n","      <th>M</th>\n","      <th>V</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2800</td>\n","      <td>-267.242706</td>\n","      <td>-43.292133</td>\n","      <td>112.838829</td>\n","      <td>0.477926</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2801</td>\n","      <td>313.012604</td>\n","      <td>-284.765137</td>\n","      <td>92.048500</td>\n","      <td>0.502950</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2802</td>\n","      <td>-233.707474</td>\n","      <td>131.553925</td>\n","      <td>30.877136</td>\n","      <td>0.399649</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2803</td>\n","      <td>159.488724</td>\n","      <td>266.318665</td>\n","      <td>28.779810</td>\n","      <td>0.426692</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2804</td>\n","      <td>-158.615936</td>\n","      <td>187.226364</td>\n","      <td>132.042160</td>\n","      <td>0.493310</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     id           X           Y           M         V\n","0  2800 -267.242706  -43.292133  112.838829  0.477926\n","1  2801  313.012604 -284.765137   92.048500  0.502950\n","2  2802 -233.707474  131.553925   30.877136  0.399649\n","3  2803  159.488724  266.318665   28.779810  0.426692\n","4  2804 -158.615936  187.226364  132.042160  0.493310"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"3QC7LF3m2DYt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593603137151,"user_tz":-180,"elapsed":616,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}}},"source":["submission.to_csv(os.path.join(save_path, 'submission_torch3.csv'), index=False)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"YKMfSSKmJhuU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"status":"ok","timestamp":1593604960752,"user_tz":-180,"elapsed":1261,"user":{"displayName":"Ioannis Meintanis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiDxdezenY2dQs0YnebK9qWlCvZfgkRVgIWX6_2=s64","userId":"13658737335839172040"}},"outputId":"8a4d78b2-27b1-4d45-f76b-d78508328363"},"source":["submission.iloc[:, 1:].hist()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f6220f21a90>,\n","        <matplotlib.axes._subplots.AxesSubplot object at 0x7f6220a8da20>],\n","       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f6220a470b8>,\n","        <matplotlib.axes._subplots.AxesSubplot object at 0x7f62209f7710>]],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":32},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x396 with 4 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"8-S6DzMEQcwV","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}
